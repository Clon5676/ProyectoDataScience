{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf8ef34",
   "metadata": {},
   "source": [
    "# Model Interation 3 (final)\n",
    "Esta es la mejor iteracion del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450f3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cdaa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yn/vbht61m97rg6wrrbgs95_9k00000gn/T/ipykernel_89962/477908195.py:2: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../Modelo/gamesWcategoricas.csv')  # ajustá el path si hace falta\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo CSV desde local\n",
    "df = pd.read_csv('../Modelo/gamesWcategoricas.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cd8da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores únicos de 'Release date':\n",
      "[2008 2017 2021 2020 2022 2014 2019 2016 2018 2010]\n",
      "\n",
      "Tipo de 'Release date': int64\n",
      "\n",
      "Valores únicos de 'Price':\n",
      "[19.99  0.99  4.99  5.99  0.   10.99  9.99 14.99  3.99 23.99]\n",
      "\n",
      "Tipo de 'Price': float64\n"
     ]
    }
   ],
   "source": [
    "# Revisar columnas clave para ingeniería\n",
    "print(\"Valores únicos de 'Release date':\")\n",
    "print(df['Release date'].unique()[:10])  # ver algunos\n",
    "\n",
    "print(\"\\nTipo de 'Release date':\", df['Release date'].dtype)\n",
    "\n",
    "print(\"\\nValores únicos de 'Price':\")\n",
    "print(df['Price'].unique()[:10])  # ver algunos\n",
    "\n",
    "print(\"\\nTipo de 'Price':\", df['Price'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "233475cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   release_year  Price price_range\n",
      "0          2008  19.99      medium\n",
      "1          2017   0.99         low\n",
      "2          2021   4.99         low\n",
      "3          2020   5.99         low\n",
      "4          2020   0.00        free\n",
      "\n",
      "Distribución de price_range:\n",
      "price_range\n",
      "low       66873\n",
      "free      23247\n",
      "medium    19066\n",
      "high       2266\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Feature: release_year\n",
    "df['release_year'] = df['Release date']\n",
    "\n",
    "# Feature: price_range\n",
    "def clasificar_precio(precio):\n",
    "    if precio == 0:\n",
    "        return 'free'\n",
    "    elif precio < 10:\n",
    "        return 'low'\n",
    "    elif precio < 30:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "df['price_range'] = df['Price'].apply(clasificar_precio)\n",
    "\n",
    "# Verificamos\n",
    "print(df[['release_year', 'Price', 'price_range']].head())\n",
    "print(\"\\nDistribución de price_range:\")\n",
    "print(df['price_range'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a080156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar One-Hot Encoding a price_range\n",
    "df_encoded = pd.get_dummies(df, columns=['price_range'], prefix='price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fce41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables predictoras\n",
    "X = df_encoded.drop(columns=['Estimated owners'])  # o el nombre de tu target\n",
    "\n",
    "# Variable objetivo\n",
    "y = df_encoded['Estimated owners']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51472ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name              object\n",
      "About the game    object\n",
      "Reviews           object\n",
      "Header image      object\n",
      "Website           object\n",
      "Support url       object\n",
      "Support email     object\n",
      "Metacritic url    object\n",
      "Notes             object\n",
      "Developers        object\n",
      "Publishers        object\n",
      "Screenshots       object\n",
      "Movies            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Verificá qué columnas no son numéricas\n",
    "print(X.dtypes[X.dtypes == 'object'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871afe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas tipo texto que no fueron codificadas\n",
    "columnas_objetables = X.select_dtypes(include='object').columns\n",
    "X = X.drop(columns=columnas_objetables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fbf4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "626d464a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# División 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalado\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenamiento\n",
    "model = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')  # para multiclase\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_iter1 = model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712dcce3",
   "metadata": {},
   "source": [
    "# Evaluación del modelo de Iteración 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0e123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión (Iteración 1):\n",
      "[[ 3398  1035     1     0     0     0     0     0     0     0     0     0\n",
      "      0]\n",
      " [  206 13568   179    33    12     6     3     3     3     2     1     0\n",
      "      0]\n",
      " [    1  1339   245    58    24    21     3     1     0     1     0     0\n",
      "      0]\n",
      " [    0   495   149    59    51    28     8     0     1     2     0     0\n",
      "      0]\n",
      " [    0   219   105    68    80    62    12     4     0     1     0     1\n",
      "      0]\n",
      " [    0    76    45    37    74   151    23    13     1     2     0     0\n",
      "      0]\n",
      " [    0     9     8     4    11    75    47    15     8     4     0     0\n",
      "      0]\n",
      " [    0     7     1     1     4    25    25    31    16     5     2     0\n",
      "      0]\n",
      " [    0     0     0     0     0     3     4    25    19     7     1     1\n",
      "      0]\n",
      " [    0     0     0     0     0     1     1     1     8     2     1     0\n",
      "      0]\n",
      " [    0     0     0     0     1     0     0     1     2     1     0     0\n",
      "      1]\n",
      " [    0     0     0     0     0     0     0     0     1     1     0     1\n",
      "      0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.77      0.85      4434\n",
      "       20000       0.81      0.97      0.88     14016\n",
      "       50000       0.33      0.14      0.20      1693\n",
      "      100000       0.23      0.07      0.11       793\n",
      "      200000       0.31      0.14      0.20       552\n",
      "      500000       0.41      0.36      0.38       422\n",
      "     1000000       0.37      0.26      0.31       181\n",
      "     2000000       0.33      0.26      0.29       117\n",
      "     5000000       0.32      0.32      0.32        60\n",
      "    10000000       0.07      0.14      0.10        14\n",
      "    20000000       0.00      0.00      0.00         6\n",
      "    50000000       0.33      0.33      0.33         3\n",
      "   100000000       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.79     22291\n",
      "   macro avg       0.34      0.29      0.31     22291\n",
      "weighted avg       0.75      0.79      0.76     22291\n",
      "\n",
      "Precision (macro): 0.343\n",
      "Recall (macro): 0.29\n",
      "F1 Score (macro): 0.305\n",
      "Precision (weighted): 0.751\n",
      "Recall (weighted): 0.79\n",
      "F1 Score (weighted): 0.759\n",
      "KS Score: 0.568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred_iter1)\n",
    "print(\"Matriz de Confusión (Iteración 1):\")\n",
    "print(cm)\n",
    "\n",
    "# Reporte general\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred_iter1))\n",
    "\n",
    "# Métricas macro y weighted\n",
    "print(\"Precision (macro):\", round(precision_score(y_test, y_pred_iter1, average='macro'), 3))\n",
    "print(\"Recall (macro):\", round(recall_score(y_test, y_pred_iter1, average='macro'), 3))\n",
    "print(\"F1 Score (macro):\", round(f1_score(y_test, y_pred_iter1, average='macro'), 3))\n",
    "\n",
    "print(\"Precision (weighted):\", round(precision_score(y_test, y_pred_iter1, average='weighted'), 3))\n",
    "print(\"Recall (weighted):\", round(recall_score(y_test, y_pred_iter1, average='weighted'), 3))\n",
    "print(\"F1 Score (weighted):\", round(f1_score(y_test, y_pred_iter1, average='weighted'), 3))\n",
    "\n",
    "\n",
    "# 1. Crear versión binaria del target para KS\n",
    "threshold = 100000\n",
    "y_test_bin = (y_test >= threshold).astype(int)\n",
    "\n",
    "# 2. Obtener índice de la clase positiva en predict_proba\n",
    "clase_positiva = model.classes_[np.argmax(model.classes_ >= threshold)]\n",
    "indice_clase = list(model.classes_).index(clase_positiva)\n",
    "\n",
    "# 3. Probabilidades predichas para esa clase\n",
    "y_prob = model.predict_proba(X_test_scaled)[:, indice_clase]\n",
    "\n",
    "# 4. Separar según valor real\n",
    "probs_pos = y_prob[y_test_bin == 1]\n",
    "probs_neg = y_prob[y_test_bin == 0]\n",
    "\n",
    "# 5. KS Score\n",
    "ks_score = ks_2samp(probs_pos, probs_neg).statistic\n",
    "print(\"KS Score:\", round(ks_score, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68cd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:3046: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.346059\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:       Estimated owners   No. Observations:                89161\n",
      "Model:                          Logit   Df Residuals:                    89157\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Fri, 16 May 2025   Pseudo R-squ.:                  0.3046\n",
      "Time:                        12:27:43   Log-Likelihood:                -30855.\n",
      "converged:                       True   LL-Null:                       -44372.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "0             -1.8476      0.012   -155.782      0.000      -1.871      -1.824\n",
      "1             -0.4273      0.012    -35.690      0.000      -0.451      -0.404\n",
      "2             -1.5026      0.011   -139.216      0.000      -1.524      -1.481\n",
      "3             -1.3483      0.015    -90.141      0.000      -1.378      -1.319\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1. Convertimos X_train escalado a DataFrame\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "\n",
    "# 2. Agregamos constante\n",
    "X_train_df_const = sm.add_constant(X_train_df)\n",
    "\n",
    "# 3. Alineamos índices\n",
    "X_train_df_const = X_train_df_const.reset_index(drop=True)\n",
    "y_binary = (y_train == 0).astype(int).reset_index(drop=True)\n",
    "\n",
    "# 4. Eliminar columnas con colinealidad (basado en condición del número)\n",
    "def remove_collinear(X, threshold=30):\n",
    "    from numpy.linalg import cond\n",
    "    while cond(X) > threshold:\n",
    "        corr_matrix = np.corrcoef(X.T)\n",
    "        np.fill_diagonal(corr_matrix, 0)\n",
    "        max_corr = np.unravel_index(np.argmax(np.abs(corr_matrix)), corr_matrix.shape)\n",
    "        X = np.delete(X, max_corr[1], axis=1)\n",
    "    return X\n",
    "\n",
    "X_array_clean = remove_collinear(X_train_df_const.values)\n",
    "\n",
    "# 5. Volvemos a convertir a DataFrame\n",
    "X_clean_df = pd.DataFrame(X_array_clean)\n",
    "\n",
    "# 6. Ajustamos el modelo\n",
    "logit_model = sm.Logit(y_binary, X_clean_df)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# 7. Mostramos resumen\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21d26e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Accuracy general del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred_iter1)\n",
    "print(\"Accuracy:\", round(accuracy, 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
