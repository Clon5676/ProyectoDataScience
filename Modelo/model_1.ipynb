{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34abad83",
   "metadata": {},
   "source": [
    "# Model Interation 1\n",
    "Esta es la primera iteracion del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604cb1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yn/vbht61m97rg6wrrbgs95_9k00000gn/T/ipykernel_91840/143770121.py:13: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../Modelo/gamesWcategoricas.csv')\n",
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Matriz de ConfusiÃ³n:\n",
      "[[4254  163   11    5    1    0    0    0    0    0    0    0    0    0]\n",
      " [1711 8819 2515  639  224   56   16   18   10    5    2    1    0    0]\n",
      " [   2  397  713  359  152   49    9    7    1    2    2    0    0    0]\n",
      " [   0  111  232  201  173   49   16    8    1    2    0    0    0    0]\n",
      " [   0   28  104   94  174   97   35   11    6    1    0    1    0    1]\n",
      " [   0    9   26   40   84  157   71   28    4    3    0    0    0    0]\n",
      " [   0    2    3    6   10   49   66   26   14    4    1    0    0    0]\n",
      " [   0    1    2    2    3   13   32   38   16    7    3    0    0    0]\n",
      " [   0    0    0    0    0    1    6   20   23    8    1    1    0    0]\n",
      " [   0    0    0    0    0    0    1    0    9    2    1    0    1    0]\n",
      " [   0    0    0    0    0    1    0    0    3    1    0    0    1    0]\n",
      " [   0    0    0    0    0    0    0    0    0    1    0    2    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "ðŸ“Œ Reporte de ClasificaciÃ³n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.713     0.959     0.818      4434\n",
      "       20000      0.925     0.629     0.749     14016\n",
      "       50000      0.198     0.421     0.269      1693\n",
      "      100000      0.149     0.253     0.188       793\n",
      "      200000      0.212     0.315     0.253       552\n",
      "      500000      0.333     0.372     0.351       422\n",
      "     1000000      0.262     0.365     0.305       181\n",
      "     2000000      0.244     0.325     0.278       117\n",
      "     5000000      0.264     0.383     0.313        60\n",
      "    10000000      0.056     0.143     0.080        14\n",
      "    20000000      0.000     0.000     0.000         6\n",
      "    50000000      0.400     0.667     0.500         3\n",
      "   100000000      0.000     0.000     0.000         0\n",
      "   200000000      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.648     22291\n",
      "   macro avg      0.268     0.345     0.293     22291\n",
      "weighted avg      0.760     0.648     0.679     22291\n",
      "\n",
      "âœ… Precision (macro): 0.268\n",
      "âœ… Recall (macro): 0.345\n",
      "âœ… F1 Score (macro): 0.293\n",
      "âœ… Precision (weighted): 0.76\n",
      "âœ… Recall (weighted): 0.648\n",
      "âœ… F1 Score (weighted): 0.679\n",
      "âœ… KS Score: 0.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/luisvillela/Desktop/UFM/2025-1/Data science/ProyectoDataScience/env/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv('../Modelo/gamesWcategoricas.csv')\n",
    "\n",
    "# IngenierÃ­a de features\n",
    "df['release_year'] = df['Release date']\n",
    "def clasificar_precio(precio):\n",
    "    if precio == 0:\n",
    "        return 'free'\n",
    "    elif precio < 10:\n",
    "        return 'low'\n",
    "    elif precio < 30:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'high'\n",
    "df['price_range'] = df['Price'].apply(clasificar_precio)\n",
    "df_encoded = pd.get_dummies(df, columns=['price_range'], prefix='price')\n",
    "\n",
    "# eparar X e y\n",
    "X = df_encoded.drop(columns=['Estimated owners'])\n",
    "y = df_encoded['Estimated owners']\n",
    "X = X.drop(columns=X.select_dtypes(include='object').columns)\n",
    "\n",
    "# Split + Escalado\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modelo\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    C=0.5,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# EvaluaciÃ³n\n",
    "print(\"Matriz de ConfusiÃ³n:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nReporte de ClasificaciÃ³n:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"Precision (macro):\", round(precision_score(y_test, y_pred, average='macro'), 3))\n",
    "print(\"Recall (macro):\", round(recall_score(y_test, y_pred, average='macro'), 3))\n",
    "print(\"F1 Score (macro):\", round(f1_score(y_test, y_pred, average='macro'), 3))\n",
    "print(\"Precision (weighted):\", round(precision_score(y_test, y_pred, average='weighted'), 3))\n",
    "print(\"Recall (weighted):\", round(recall_score(y_test, y_pred, average='weighted'), 3))\n",
    "print(\"F1 Score (weighted):\", round(f1_score(y_test, y_pred, average='weighted'), 3))\n",
    "\n",
    "# KS Score\n",
    "threshold = 100000\n",
    "y_test_bin = (y_test >= threshold).astype(int)\n",
    "clase_positiva = model.classes_[np.argmax(model.classes_ >= threshold)]\n",
    "indice_clase = list(model.classes_).index(clase_positiva)\n",
    "y_prob = model.predict_proba(X_test_scaled)[:, indice_clase]\n",
    "probs_pos = y_prob[y_test_bin == 1]\n",
    "probs_neg = y_prob[y_test_bin == 0]\n",
    "ks_score = ks_2samp(probs_pos, probs_neg).statistic\n",
    "print(\"KS Score:\", round(ks_score, 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
